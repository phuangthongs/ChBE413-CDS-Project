{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca3b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import CGConv, NNConv, Set2Set\n",
    "from torch_geometric.nn.aggr import AttentionalAggregation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "import rdkit.Chem.rdchem as rdc\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26287a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_utils import *\n",
    "from feature_importance import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6ebde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels/Hyperparameter definition\n",
    "\n",
    "target_cols = [\n",
    "    \"CONDUCTIVITY\",\n",
    "    \"TFSI Diffusivity\",\n",
    "    \"Li Diffusivity\",\n",
    "    \"Poly Diffusivity\",\n",
    "    \"Transference Number\",\n",
    "]\n",
    "\n",
    "csv_path = 'data/simulation-trajectory-aggregate.csv'\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 60\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "save_dir = \"model_out\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a45def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup feature ranges based on SMILES strings\n",
    "\n",
    "def compute_feature_ranges(list_of_smiles):\n",
    "    atom_symbols = set()\n",
    "    degrees = set()\n",
    "    formal_charges = set()\n",
    "    num_hs = set()\n",
    "    hybridizations = set()\n",
    "    \n",
    "    bond_types = set()\n",
    "    bond_stereo = set()\n",
    "    conjugations = set()\n",
    "\n",
    "    for smile in list_of_smiles:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        # mol_atoms = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_symbols.add(atom.GetSymbol())\n",
    "            degrees.add(atom.GetDegree())\n",
    "            formal_charges.add(atom.GetFormalCharge())\n",
    "            num_hs.add(atom.GetTotalNumHs())\n",
    "            hybridizations.add(str(atom.GetHybridization()))\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            bond_types.add(str(bond.GetBondType()))\n",
    "            bond_stereo.add(str(bond.GetStereo()))\n",
    "            conjugations.add(bond.GetIsConjugated())\n",
    "    \n",
    "    x_map = {\n",
    "        \"atomic_symbol\": sorted(atom_symbols),\n",
    "        \"degree\": sorted(degrees),\n",
    "        \"formal_charge\": sorted(formal_charges),\n",
    "        \"num_hs\": sorted(num_hs),\n",
    "        \"hybridization\": sorted(hybridizations),\n",
    "        \"is_aromatic\": [False, True],\n",
    "        \"is_in_ring\": [False, True],\n",
    "    }\n",
    "\n",
    "    e_map = {\n",
    "        \"bond_type\": sorted(bond_types),\n",
    "        \"stereo\": sorted(bond_stereo),\n",
    "        \"is_conjugated\": [False, True],\n",
    "    }\n",
    "    return x_map, e_map\n",
    "\n",
    "def add_unknowns(x_map: dict, e_map: dict):\n",
    "    # Add UNK token to categorical lists if not present\n",
    "    for k in [\"atomic_symbol\", \"hybridization\"]:\n",
    "        if \"UNK\" not in x_map[k]:\n",
    "            x_map[k].append(\"UNK\")\n",
    "    # For degree/formal_charge/num_hs: we will keep them as discrete sets but\n",
    "    # if unseen values appear we will map to a min/max or UNK index — we append 'UNK' as str to be safe.\n",
    "    for k in [\"degree\", \"formal_charge\", \"num_hs\"]:\n",
    "        # convert to strings? we keep them as numbers but ensure UNK presence for one_hot implementation\n",
    "        if \"UNK\" not in x_map[k]:\n",
    "            x_map[k] = list(x_map[k]) + [\"UNK\"]\n",
    "\n",
    "    for k in [\"bond_type\", \"stereo\"]:\n",
    "        if \"UNK\" not in e_map[k]:\n",
    "            e_map[k].append(\"UNK\")\n",
    "\n",
    "    # ensure boolean lists contain False/True\n",
    "    x_map['is_aromatic'] = [False, True]\n",
    "    x_map['is_in_ring'] = [False, True]\n",
    "    e_map['is_conjugated'] = [False, True]\n",
    "\n",
    "    return x_map, e_map\n",
    "\n",
    "def get_bond_feature_dim(e_map):\n",
    "    dim = (\n",
    "        len(e_map[\"bond_type\"]) +\n",
    "        len(e_map[\"stereo\"]) +\n",
    "        len(e_map[\"is_conjugated\"])\n",
    "    )\n",
    "    return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Definition: one-hot, atom (node), bond (edge), generate graph from SMILES\n",
    "\n",
    "def one_hot(x, choices):\n",
    "    if x not in choices:\n",
    "        x = \"UNK\"\n",
    "    return np.array([x == c for c in choices], dtype=float)\n",
    "\n",
    "def atom_features(atom, x_map):\n",
    "    symbol = atom.GetSymbol()\n",
    "    degree = atom.GetDegree()\n",
    "    fc = atom.GetFormalCharge()\n",
    "    hs = atom.GetTotalNumHs()\n",
    "    hyb = str(atom.GetHybridization())\n",
    "    aromatic = atom.GetIsAromatic()\n",
    "    ring = atom.IsInRing()\n",
    "\n",
    "    return np.concatenate([\n",
    "        one_hot(symbol, x_map[\"atomic_symbol\"]),\n",
    "        one_hot(degree, x_map[\"degree\"]),\n",
    "        one_hot(fc, x_map[\"formal_charge\"]),\n",
    "        one_hot(hs, x_map[\"num_hs\"]),\n",
    "        one_hot(hyb, x_map[\"hybridization\"]),\n",
    "        one_hot(aromatic, x_map[\"is_aromatic\"]),\n",
    "        one_hot(ring, x_map[\"is_in_ring\"]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def bond_features(bond, e_map):\n",
    "    btype = str(bond.GetBondType())\n",
    "    stereo = str(bond.GetStereo())\n",
    "    conj = bond.GetIsConjugated()\n",
    "\n",
    "    return np.concatenate([\n",
    "        one_hot(btype, e_map[\"bond_type\"]),\n",
    "        one_hot(stereo, e_map[\"stereo\"]),\n",
    "        one_hot(conj, e_map[\"is_conjugated\"]),\n",
    "    ])\n",
    "\n",
    "def smiles_to_graph(smiles, x_map, e_map, add_h: bool = True, add_self_loops=True):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "    if add_h:\n",
    "        mol = Chem.AddHs(mol)\n",
    "    \n",
    "    atom_feats = []\n",
    "    edge_index = []\n",
    "    edge_feats = []\n",
    "\n",
    "    # Atom features\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_feats.append(atom_features(atom, x_map))\n",
    "\n",
    "    # Bond features\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        bf = bond_features(bond, e_map)\n",
    "        edge_index.append([i, j])\n",
    "        edge_feats.append(bf)\n",
    "\n",
    "        edge_index.append([j, i])\n",
    "        edge_feats.append(bf)\n",
    "\n",
    "    if add_self_loops:\n",
    "        bond_dim = (\n",
    "            len(e_map[\"bond_type\"]) +\n",
    "            len(e_map[\"stereo\"]) +\n",
    "            len(e_map[\"is_conjugated\"])\n",
    "        )\n",
    "        zero_bond = np.zeros(bond_dim, dtype=float)\n",
    "\n",
    "        num_atoms = len(atom_feats)\n",
    "        for i in range(num_atoms):\n",
    "            edge_index.append([i, i])\n",
    "            edge_feats.append(zero_bond)\n",
    "\n",
    "    x = torch.tensor(np.array(atom_feats), dtype=torch.float)\n",
    "    edge_index = torch.tensor(np.array(edge_index).T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(np.array(edge_feats), dtype=torch.float)\n",
    "    \n",
    "    g = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    g.num_nodes = x.size(0)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36037018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CSV file, generate graphs\n",
    "\n",
    "class PolymerCSV(InMemoryDataset):\n",
    "    def __init__(self, csv_path: str, target_cols: list[str], x_map: dict, e_map: dict, add_h: bool = True):\n",
    "        super().__init__(None)\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.target_cols = target_cols\n",
    "        self.x_map = x_map\n",
    "        self.e_map = e_map\n",
    "        self.add_h = add_h\n",
    "\n",
    "        data_list = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            smi = row.get(\"Mol smiles\")\n",
    "            if pd.isna(smi):\n",
    "                continue\n",
    "            try:\n",
    "                g = smiles_to_graph(smi, self.x_map, self.e_map, self.add_h, True)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Skipping SMILES {smi}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # collect targets and mask\n",
    "            y = []\n",
    "            mask = []\n",
    "            for c in self.target_cols:\n",
    "                val = row.get(c, np.nan) # get column value or NaN if unavailable\n",
    "                if pd.isna(val):\n",
    "                    y.append(0.0)\n",
    "                    mask.append(0.0)\n",
    "                else:\n",
    "                    y.append(float(val))\n",
    "                    mask.append(1.0)\n",
    "\n",
    "            g.y = torch.tensor([y], dtype=torch.float)\n",
    "            g.y_mask = torch.tensor([mask], dtype=torch.float)\n",
    "            \n",
    "            # optional: save trajectory id as meta\n",
    "            if \"Trajectory ID\" in row:\n",
    "                g.traj_id = int(row[\"Trajectory ID\"])\n",
    "            data_list.append(g)\n",
    "\n",
    "        self.data, self.slices = self.collate(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitask CGCNN setup\n",
    "\n",
    "class MultiTaskCGCNN(nn.Module):\n",
    "    def __init__(self, node_in: int, edge_in: int, fea_dim=96, n_layers=4, n_hidden=2, out_dim=5):\n",
    "        super().__init__()\n",
    "        self.node_embed = Linear(node_in, fea_dim)\n",
    "        self.edge_embed = Linear(edge_in, fea_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            CGConv(channels=fea_dim, dim=fea_dim, aggr=\"mean\", batch_norm=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.pool = AttentionalAggregation(\n",
    "            gate_nn=Sequential(Linear(fea_dim, fea_dim), Linear(fea_dim, 1)),\n",
    "            nn=Sequential(Linear(fea_dim, fea_dim), Linear(fea_dim, fea_dim))\n",
    "        )\n",
    "\n",
    "        self.hiddens = nn.ModuleList([Linear(fea_dim, fea_dim) for _ in range(n_hidden)])\n",
    "        self.head = Linear(fea_dim, out_dim)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = F.leaky_relu(self.node_embed(data.x))\n",
    "        e = F.leaky_relu(self.edge_embed(data.edge_attr))\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, data.edge_index, e)\n",
    "        g = self.pool(x, data.batch)\n",
    "        for h in self.hiddens:\n",
    "            g = F.leaky_relu(h(g))\n",
    "        out = self.head(g)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked MSE & MAE\n",
    "\n",
    "def masked_mse_loss(pred: torch.Tensor, y: torch.Tensor, mask: torch.Tensor):\n",
    "    # pred,y,mask -> [B, T]\n",
    "    \n",
    "    diff = (pred - y) * mask\n",
    "    denom = mask.sum()\n",
    "    if denom.item() == 0:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "    return (diff * diff).sum() / denom\n",
    "\n",
    "\n",
    "def masked_mae(pred: np.ndarray, y: np.ndarray, mask: np.ndarray):\n",
    "    # numpy arrays\n",
    "    diff = np.abs(pred - y) * mask\n",
    "    denom = mask.sum()\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return diff.sum() / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate epochs\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler: StandardScaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        # scale batch targets (y) using scaler trained on train set\n",
    "        # scaler expects 2D arrays; convert\n",
    "        y_cpu = batch.y.cpu().numpy()\n",
    "        # IMPORTANT: scaler.transform requires shape [n_samples, n_targets]\n",
    "        # print(np.shape(y_cpu))\n",
    "        y_scaled = torch.tensor(scaler.transform(y_cpu), dtype=torch.float, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)  # [B, T]\n",
    "        loss = masked_mse_loss(out, y_scaled, batch.y_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "    \n",
    "    loss = total_loss / len(loader.dataset)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, scaler: StandardScaler, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_r2 = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_ys = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            y_cpu = batch.y.cpu().numpy()\n",
    "            y_scaled = torch.tensor(scaler.transform(y_cpu), dtype=torch.float, device=device)\n",
    "\n",
    "            out_scaled = model(batch)\n",
    "            loss = masked_mse_loss(out_scaled, y_scaled, batch.y_mask)\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            # collect predictions and ground truth in original scale for MAE\n",
    "            out_unscaled = scaler.inverse_transform(out_scaled.cpu().numpy())\n",
    "            y_unscaled = batch.y.cpu().numpy()  # raw y already unscaled\n",
    "\n",
    "            all_preds.append(out_unscaled)\n",
    "            all_ys.append(y_unscaled)\n",
    "\n",
    "            all_masks.append(batch.y_mask.cpu().numpy())\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return total_loss / max(1, len(loader.dataset)), None\n",
    "\n",
    "    preds = np.vstack(all_preds)\n",
    "    ys = np.vstack(all_ys)\n",
    "    masks = np.vstack(all_masks)\n",
    "\n",
    "\n",
    "    # per task loss/MAE\n",
    "    T = preds.shape[1]\n",
    "    per_task_mae = []\n",
    "    per_task_r2 = []\n",
    "\n",
    "    for t in range(T):\n",
    "        mask_t = masks[:, t].astype(bool)\n",
    "\n",
    "        if mask_t.sum() == 0:\n",
    "            per_task_mae.append(None)\n",
    "            per_task_r2.append(None)\n",
    "            continue\n",
    "\n",
    "        pred_t = preds[:, t][mask_t]\n",
    "        y_t = ys[:, t][mask_t]\n",
    "\n",
    "        mae_t = np.mean(np.abs(pred_t - y_t))\n",
    "        per_task_mae.append(mae_t)\n",
    "\n",
    "        if len(y_t) > 1:\n",
    "            per_task_r2.append(r2_score(y_t, pred_t))\n",
    "        else:\n",
    "            per_task_r2.append(None)\n",
    "            \n",
    "    # compute global loss/MAE across all tasks using masking\n",
    "    overall_loss = total_loss / len(loader.dataset)\n",
    "    overall_mae = masked_mae(preds, ys, masks)\n",
    "    overall_r2 = np.mean(per_task_r2)\n",
    "\n",
    "    return overall_loss, overall_mae, overall_r2, per_task_mae, per_task_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f754b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation, testing splits\n",
    "\n",
    "def create_splits(df: pd.DataFrame, frac_train=0.8, frac_val=0.125):\n",
    "    n = len(df)\n",
    "    test_size = 1.0 - frac_train\n",
    "    idx = np.arange(n)\n",
    "    train_idx, test_idx = train_test_split(idx, test_size=test_size)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=frac_val / frac_train)\n",
    "\n",
    "    return train_idx.tolist(), val_idx.tolist(), test_idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build, train, and test model\n",
    "\n",
    "def model_execution():\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) Load CSV + build feature maps\n",
    "    # ============================================================\n",
    "    df = read_no_rescale()\n",
    "    if \"Mol smiles\" not in df.columns and \"Mol_smiles\" in df.columns:\n",
    "        df = df.rename(columns={\"Mol_smiles\": \"Mol smiles\"})\n",
    "    smiles_list = df[\"Mol smiles\"].dropna().astype(str).tolist()\n",
    "\n",
    "    print(f\"Found {len(smiles_list)} SMILES in CSV.\")\n",
    "\n",
    "    x_map, e_map = compute_feature_ranges(smiles_list)\n",
    "    x_map, e_map = add_unknowns(x_map, e_map)\n",
    "\n",
    "    print(\"\\n ===> Atom feature map sizes <===\")\n",
    "    for k, v in x_map.items():\n",
    "        print(f\"{k:>15}: {len(v)}\")\n",
    "\n",
    "    print(\"\\n ===> Edge feature map sizes <===\")\n",
    "    for k, v in e_map.items():\n",
    "        print(f\"{k:>15}: {len(v)}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) Train/val/test splits\n",
    "    # ============================================================\n",
    "    train_idx, val_idx, test_idx = create_splits(df, frac_train=0.8, frac_val=0.1)\n",
    "    print(f\"\\nSplit sizes: train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")\n",
    "\n",
    "    dataset = PolymerCSV(csv_path, target_cols, x_map=x_map, e_map=e_map, add_h=True)\n",
    "    print(\"\\nTotal graphs built:\", len(dataset))\n",
    "\n",
    "    train_ds = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_ds   = torch.utils.data.Subset(dataset, val_idx)\n",
    "    test_ds  = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) Fit scaler only on training targets\n",
    "    # ============================================================\n",
    "    def collect_targets(subset):\n",
    "        Ys = [g.y.numpy() for g in subset]\n",
    "        return np.vstack(Ys) if Ys else np.zeros((0, len(target_cols)))\n",
    "\n",
    "    y_train = collect_targets(train_ds)\n",
    "    if y_train.shape[0] == 0:\n",
    "        raise RuntimeError(\"No training examples with targets found!\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(y_train)\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) Build model + optimizer\n",
    "    # ============================================================\n",
    "    sample = dataset[0]\n",
    "    node_in = sample.x.shape[1]\n",
    "    edge_in = sample.edge_attr.shape[1]\n",
    "    out_dim = len(target_cols)\n",
    "\n",
    "    model = MultiTaskCGCNN(\n",
    "        node_in=node_in,\n",
    "        edge_in=edge_in,\n",
    "        fea_dim=96,\n",
    "        n_layers=4,\n",
    "        n_hidden=2,\n",
    "        out_dim=out_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) Unified metrics structure\n",
    "    # ============================================================\n",
    "    metrics = {\n",
    "        \"train\": {\n",
    "            \"loss\": [],\n",
    "            \"mae\":  [],\n",
    "            \"r2\":   [],\n",
    "            \"per_task_mae\": {t: [] for t in target_cols},\n",
    "            \"per_task_r2\":  {t: [] for t in target_cols},\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"loss\": [],\n",
    "            \"mae\":  [],\n",
    "            \"r2\":   [],\n",
    "            \"per_task_mae\": {t: [] for t in target_cols},\n",
    "            \"per_task_r2\":  {t: [] for t in target_cols},\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"loss\": None,\n",
    "            \"mae\":  None,\n",
    "            \"r2\":   None,\n",
    "            \"per_task_mae\": {t: None for t in target_cols},\n",
    "            \"per_task_r2\":  {t: None for t in target_cols},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ============================================================\n",
    "    # 6) Training loop\n",
    "    # ============================================================\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "\n",
    "        # ---- training ----\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        tr_loss, tr_mae, tr_r2, tr_pt_mae, tr_pt_r2 = eval_epoch(\n",
    "            model, train_loader, scaler, device\n",
    "        )\n",
    "        tr_loss = train_loss   # override with actual loss from train_epoch\n",
    "\n",
    "        # ---- validation ----\n",
    "        val_loss, val_mae, val_r2, val_pt_mae, val_pt_r2 = eval_epoch(\n",
    "            model, val_loader, scaler, device\n",
    "        )\n",
    "\n",
    "        # ---- store metrics ----\n",
    "        metrics[\"train\"][\"loss\"].append(tr_loss)\n",
    "        metrics[\"train\"][\"mae\"].append(tr_mae)\n",
    "        metrics[\"train\"][\"r2\"].append(tr_r2)\n",
    "\n",
    "        metrics[\"val\"][\"loss\"].append(val_loss)\n",
    "        metrics[\"val\"][\"mae\"].append(val_mae)\n",
    "        metrics[\"val\"][\"r2\"].append(val_r2)\n",
    "\n",
    "        for i, t in enumerate(target_cols):\n",
    "            metrics[\"train\"][\"per_task_mae\"][t].append(tr_pt_mae[i])\n",
    "            metrics[\"train\"][\"per_task_r2\"][t].append(tr_pt_r2[i])\n",
    "            metrics[\"val\"][\"per_task_mae\"][t].append(val_pt_mae[i])\n",
    "            metrics[\"val\"][\"per_task_r2\"][t].append(val_pt_r2[i])\n",
    "\n",
    "        # ---- checkpoint ----\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"scaler\": scaler,\n",
    "                    \"x_map\": x_map,\n",
    "                    \"e_map\": e_map,\n",
    "                    \"metrics\": metrics,\n",
    "                },\n",
    "                best_path,\n",
    "            )\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) Final test evaluation\n",
    "    # ============================================================\n",
    "    ckpt = torch.load(best_path, map_location=device, weights_only=False)\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    scaler = ckpt[\"scaler\"]\n",
    "\n",
    "    test_loss, test_mae, test_r2, test_pt_mae, test_pt_r2 = eval_epoch(\n",
    "        model, test_loader, scaler, device\n",
    "    )\n",
    "\n",
    "    metrics[\"test\"][\"loss\"] = test_loss\n",
    "    metrics[\"test\"][\"mae\"]  = test_mae\n",
    "    metrics[\"test\"][\"r2\"]   = test_r2\n",
    "    for i, t in enumerate(target_cols):\n",
    "        metrics[\"test\"][\"per_task_mae\"][t] = test_pt_mae[i]\n",
    "        metrics[\"test\"][\"per_task_r2\"][t]  = test_pt_r2[i]\n",
    "\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics for final model\n",
    "\n",
    "def plot_all_metrics(metrics, target_cols, num_epochs):\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. GLOBAL LOSS\n",
    "    # ============================================================\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, metrics[\"train\"][\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, metrics[\"val\"][\"loss\"],   label=\"Val Loss\")\n",
    "    plt.scatter([num_epochs], [metrics[\"test\"][\"loss\"]], color=\"red\", label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scaled MSE Loss\")\n",
    "    plt.title(\"Global Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 2. GLOBAL MAE\n",
    "    # ============================================================\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, metrics[\"train\"][\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(epochs, metrics[\"val\"][\"mae\"],   label=\"Val MAE\")\n",
    "    plt.scatter([num_epochs], [metrics[\"test\"][\"mae\"]], color=\"red\", label=\"Test MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"Global MAE\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 3. GLOBAL R2\n",
    "    # ============================================================\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, metrics[\"train\"][\"r2\"], label=\"Train R²\")\n",
    "    plt.plot(epochs, metrics[\"val\"][\"r2\"],   label=\"Val R²\")\n",
    "    plt.scatter([num_epochs], [metrics[\"test\"][\"r2\"]], color=\"red\", label=\"Test R²\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"R² Score\")\n",
    "    plt.title(\"Global R²\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 4. PER-TASK MAE SUBPLOTS\n",
    "    # ============================================================\n",
    "    T = len(target_cols)\n",
    "    ncols = math.ceil(math.sqrt(T))\n",
    "    nrows = math.ceil(T / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, t in enumerate(target_cols):\n",
    "        ax = axes[idx]\n",
    "        train_vals = metrics[\"train\"][\"per_task_mae\"][t]\n",
    "        val_vals   = metrics[\"val\"][\"per_task_mae\"][t]\n",
    "        test_val   = metrics[\"test\"][\"per_task_mae\"][t]\n",
    "\n",
    "        ax.plot(epochs, train_vals, label=\"Train\", linewidth=2)\n",
    "        ax.plot(epochs, val_vals,   label=\"Val\",   linewidth=2)\n",
    "        if test_val is not None:\n",
    "            ax.scatter([num_epochs], [test_val], color=\"red\", s=40, label=\"Test\")\n",
    "\n",
    "        ax.set_title(f\"{t} — MAE\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"MAE\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    # hide any unused axes\n",
    "    for j in range(idx+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.suptitle(\"Per-Task MAE Over Time\", fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 5. PER-TASK R2 SUBPLOTS\n",
    "    # ============================================================\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, t in enumerate(target_cols):\n",
    "        ax = axes[idx]\n",
    "        ax.grid(True)\n",
    "\n",
    "        train_vals = metrics[\"train\"][\"per_task_r2\"][t]\n",
    "        val_vals   = metrics[\"val\"][\"per_task_r2\"][t]\n",
    "        test_val   = metrics[\"test\"][\"per_task_r2\"][t]\n",
    "\n",
    "        ax.plot(epochs, train_vals, label=\"Train\", linewidth=2)\n",
    "        ax.plot(epochs, val_vals,   label=\"Val\",   linewidth=2)\n",
    "        if test_val is not None:\n",
    "            ax.scatter([num_epochs], [test_val], color=\"red\", s=40, label=\"Test\")\n",
    "\n",
    "        ax.set_title(f\"{t} — R²\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"R²\")\n",
    "        ax.legend()\n",
    "\n",
    "    # hide any unused axes\n",
    "    for j in range(idx+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    fig.suptitle(\"Per-Task R² Over Time\", fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6270 SMILES in CSV.\n",
      "\n",
      " ===> Atom feature map sizes <===\n",
      "  atomic_symbol: 11\n",
      "         degree: 5\n",
      "  formal_charge: 4\n",
      "         num_hs: 5\n",
      "  hybridization: 5\n",
      "    is_aromatic: 2\n",
      "     is_in_ring: 2\n",
      "\n",
      " ===> Edge feature map sizes <===\n",
      "      bond_type: 5\n",
      "         stereo: 2\n",
      "  is_conjugated: 2\n",
      "\n",
      "Split sizes: train=4389 val=627 test=1254\n",
      "\n",
      "Total graphs built: 6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/60 [00:12<00:36,  1.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, metrics = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m plot_all_metrics(metrics, target_cols, num_epochs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# 6) Training loop\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m)):\n\u001b[32m    108\u001b[39m \n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# ---- training ----\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     tr_loss, tr_mae, tr_r2, tr_pt_mae, tr_pt_r2 = eval_epoch(\n\u001b[32m    112\u001b[39m         model, train_loader, scaler, device\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    114\u001b[39m     tr_loss = train_loss   \u001b[38;5;66;03m# override with actual loss from train_epoch\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scaler, device)\u001b[39m\n\u001b[32m     14\u001b[39m out = model(batch)  \u001b[38;5;66;03m# [B, T]\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = masked_mse_loss(out, y_scaled, batch.y_mask)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m optimizer.step()\n\u001b[32m     18\u001b[39m total_loss += loss.item() * batch.num_graphs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/keras_pytorch/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/keras_pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/keras_pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "final_model, final_metrics = model_execution()\n",
    "plot_all_metrics(final_metrics, target_cols, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
